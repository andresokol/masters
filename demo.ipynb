{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "import re\n",
    "import typing as tp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# cuda_available = torch.cuda.is_available()\n",
    "# dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Tensor = torch.Tensor\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "FFHQ_DIR = \"/home/andresokol/data/compressed\"\n",
    "STRUCTURE_ROOT = f\"/home/andresokol/data/orientation_v2\"\n",
    "RENDERED_ROOT = f\"/home/andresokol/rendered/v1\"\n",
    "MASK_ROOT = f\"/home/andresokol/data/masks_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        def _block(channels_in, channels_out):\n",
    "            return [\n",
    "                nn.Conv2d(channels_in, channels_out, kernel_size=3, padding=1, stride=2),\n",
    "                nn.BatchNorm2d(channels_out),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            ]\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *_block(3, 6),\n",
    "            *_block(6, 12),\n",
    "            *_block(12, 24),\n",
    "            *_block(24, 48),\n",
    "            *_block(48, 96),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch version of ResNet\n",
    "# https://github.com/pytorch/vision/blob/cddad9ca3822011548e18342f52a3e9f4724c2dd/torchvision/models/resnet.py#L88\n",
    "\n",
    "\n",
    "class ResBlk(nn.Module):\n",
    "    def __init__(self, channels) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(channels)\n",
    "        self.conv_2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.conv_1(x)\n",
    "        out = self.batch_norm_1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "\n",
    "        out += x\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# E_f = nn.Sequential(\n",
    "#     DownsampleBlock(),\n",
    "#     *(ResBlk() for _ in range(4)),\n",
    "# ).to(device)\n",
    "\n",
    "# E_r = nn.Sequential(\n",
    "#     DownsampleBlock(),\n",
    "#     *(ResBlk() for _ in range(4)),\n",
    "# ).to(device)\n",
    "\n",
    "# E_ = nn.Sequential(\n",
    "#     ResBlk(),\n",
    "#     ResBlk(),\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GeneratorFake(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        def _upscale_conv(channels_in, channels_out):\n",
    "            return (\n",
    "                nn.ConvTranspose2d(channels_in, channels_out, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(channels_out),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "\n",
    "        self.resblk1 = ResBlk(96)\n",
    "        self.resblk2 = ResBlk(96)\n",
    "        self.upscale_layers = nn.Sequential(\n",
    "            *_upscale_conv(96, 48),\n",
    "            *_upscale_conv(48, 24),\n",
    "            *_upscale_conv(24, 12),\n",
    "            *_upscale_conv(12, 6),\n",
    "            *_upscale_conv(6, 3),\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.resblk1(x)\n",
    "        out = self.resblk2(out)\n",
    "        out = self.upscale_layers(out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        # for i in range(5):\n",
    "        #     out = self.upscale_layers[i](out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        return torch.tanh(self.out_conv(out))\n",
    "\n",
    "\n",
    "# G_f = GeneratorFake().to(device)\n",
    "# G_f = nn.Sequential(\n",
    "#     *(ResBlk() for _ in range(2)),\n",
    "#     *(nn.ConvTranspose2d(3, 3, kernel_size=3, stride=2, padding=1) for _ in range(5)),\n",
    "#     # nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from https://github.com/NVlabs/SPADE\n",
    "\n",
    "# Creates SPADE normalization layer based on the given configuration\n",
    "# SPADE consists of two steps. First, it normalizes the activations using\n",
    "# your favorite normalization method, such as Batch Norm or Instance Norm.\n",
    "# Second, it applies scale and bias to the normalized output, conditioned on\n",
    "# the segmentation map.\n",
    "# The format of |config_text| is spade(norm)(ks), where\n",
    "# (norm) specifies the type of parameter-free normalization.\n",
    "#       (e.g. syncbatch, batch, instance)\n",
    "# (ks) specifies the size of kernel in the SPADE module (e.g. 3x3)\n",
    "# Example |config_text| will be spadesyncbatch3x3, or spadeinstance5x5.\n",
    "# Also, the other arguments are\n",
    "# |norm_nc|: the #channels of the normalized activations, hence the output dim of SPADE\n",
    "# |label_nc|: the #channels of the input semantic map, hence the input dim of SPADE\n",
    "class SPADE(nn.Module):\n",
    "    def __init__(self, config_text, norm_nc, label_nc):\n",
    "        super().__init__()\n",
    "\n",
    "        assert config_text.startswith('spade')\n",
    "        parsed = re.search('spade(\\D+)(\\d)x\\d', config_text)\n",
    "        param_free_norm_type = str(parsed.group(1))\n",
    "        ks = int(parsed.group(2))\n",
    "\n",
    "        if param_free_norm_type == 'instance':\n",
    "            self.param_free_norm = nn.InstanceNorm2d(norm_nc, affine=False)\n",
    "        # elif param_free_norm_type == 'syncbatch':\n",
    "        #     self.param_free_norm = SynchronizedBatchNorm2d(norm_nc, affine=False)\n",
    "        elif param_free_norm_type == 'batch':\n",
    "            self.param_free_norm = nn.BatchNorm2d(norm_nc, affine=False)\n",
    "        else:\n",
    "            raise ValueError('%s is not a recognized param-free norm type in SPADE'\n",
    "                             % param_free_norm_type)\n",
    "\n",
    "        # The dimension of the intermediate embedding space. Yes, hardcoded.\n",
    "        nhidden = 128\n",
    "\n",
    "        pw = ks // 2\n",
    "        self.mlp_shared = nn.Sequential(\n",
    "            nn.Conv2d(label_nc, nhidden, kernel_size=ks, padding=pw),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mlp_gamma = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n",
    "        self.mlp_beta = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n",
    "\n",
    "    def forward(self, x, segmap):\n",
    "\n",
    "        # Part 1. generate parameter-free normalized activations\n",
    "        normalized = self.param_free_norm(x)\n",
    "\n",
    "        # Part 2. produce scaling and bias conditioned on semantic map\n",
    "        segmap = F.interpolate(segmap, size=x.size()[2:], mode='nearest')\n",
    "        actv = self.mlp_shared(segmap)\n",
    "        gamma = self.mlp_gamma(actv)\n",
    "        beta = self.mlp_beta(actv)\n",
    "\n",
    "        # apply scale and bias\n",
    "        out = normalized * (1 + gamma) + beta\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# ResNet block that uses SPADE.\n",
    "# It differs from the ResNet block of pix2pixHD in that\n",
    "# it takes in the segmentation map as input, learns the skip connection if necessary,\n",
    "# and applies normalization first and then convolution.\n",
    "# This architecture seemed like a standard architecture for unconditional or\n",
    "# class-conditional GAN architecture using residual block.\n",
    "# The code was inspired from https://github.com/LMescheder/GAN_stability.\n",
    "class SPADEResnetBlock(nn.Module):\n",
    "    def __init__(self, fin, fout, segmap_channels):\n",
    "        super().__init__()\n",
    "        # Attributes\n",
    "        self.learned_shortcut = (fin != fout)\n",
    "        fmiddle = min(fin, fout)\n",
    "        \n",
    "        # create conv layers\n",
    "        self.conv_0 = nn.Conv2d(fin, fmiddle, kernel_size=3, padding=1)\n",
    "        self.conv_1 = nn.Conv2d(fmiddle, fout, kernel_size=3, padding=1)\n",
    "        if self.learned_shortcut:\n",
    "            self.conv_s = nn.Conv2d(fin, fout, kernel_size=1, bias=False)\n",
    "\n",
    "        # apply spectral norm if specified\n",
    "        # if 'spectral' in opt.norm_G:\n",
    "        #     self.conv_0 = spectral_norm(self.conv_0)\n",
    "        #     self.conv_1 = spectral_norm(self.conv_1)\n",
    "        #     if self.learned_shortcut:\n",
    "        #         self.conv_s = spectral_norm(self.conv_s)\n",
    "\n",
    "        # define normalization layers\n",
    "#         spade_config_str = opt.norm_G.replace('spectral', '')\n",
    "        spade_config_str = \"spadebatch3x3\"\n",
    "        self.norm_0 = SPADE(spade_config_str, fin, segmap_channels)\n",
    "        self.norm_1 = SPADE(spade_config_str, fmiddle, segmap_channels)\n",
    "        if self.learned_shortcut:\n",
    "            self.norm_s = SPADE(spade_config_str, fin, segmap_channels)\n",
    "\n",
    "    # note the resnet block with SPADE also takes in |seg|,\n",
    "    # the semantic segmentation map as input\n",
    "    def forward(self, x, seg):\n",
    "        x_s = self.shortcut(x, seg)\n",
    "\n",
    "        dx = self.conv_0(self.actvn(self.norm_0(x, seg)))\n",
    "        dx = self.conv_1(self.actvn(self.norm_1(dx, seg)))\n",
    "\n",
    "        out = x_s + dx\n",
    "\n",
    "        return out\n",
    "\n",
    "    def shortcut(self, x, seg):\n",
    "        if self.learned_shortcut:\n",
    "            x_s = self.conv_s(self.norm_s(x, seg))\n",
    "        else:\n",
    "            x_s = x\n",
    "        return x_s\n",
    "\n",
    "    def actvn(self, x):\n",
    "        return F.leaky_relu(x, 2e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GeneratorReal(nn.Module):\n",
    "    def __init__(self, mat_features_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resblk1 = ResBlk(96)\n",
    "        self.resblk2 = ResBlk(96)\n",
    "\n",
    "        self.spade1 = SPADEResnetBlock(96, 48, mat_features_channels)\n",
    "        self.spade2 = SPADEResnetBlock(48, 24, mat_features_channels)\n",
    "        self.spade3 = SPADEResnetBlock(24, 12, mat_features_channels)\n",
    "        self.spade4 = SPADEResnetBlock(12, 6, mat_features_channels)\n",
    "        self.spade5 = SPADEResnetBlock(6, 3, mat_features_channels)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.out_conv = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: Tensor, seg: Tensor) -> Tensor:\n",
    "        out = self.resblk1(x)\n",
    "        out = self.resblk2(out)\n",
    "\n",
    "        out = self.upsample(out)\n",
    "        out = self.spade1(out, seg)\n",
    "\n",
    "        out = self.upsample(out)\n",
    "        out = self.spade2(out, seg)\n",
    "        \n",
    "        out = self.upsample(out)\n",
    "        out = self.spade3(out, seg)\n",
    "        \n",
    "        out = self.upsample(out)\n",
    "        out = self.spade4(out, seg)\n",
    "        \n",
    "        out = self.upsample(out)\n",
    "        out = self.spade5(out, seg)\n",
    "\n",
    "        out = self.activation(out)\n",
    "        out = self.out_conv(out)\n",
    "        \n",
    "        return torch.tanh(out)\n",
    "\n",
    "# G_r = GeneratorReal().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderMaterial(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def _block(channels_in, channels_out):\n",
    "            return [\n",
    "                nn.Conv2d(channels_in, channels_out, kernel_size=3, padding=1, stride=2),\n",
    "                nn.BatchNorm2d(channels_out),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            ]\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *_block(3, 6),\n",
    "            *_block(6, 12),\n",
    "            *_block(12, 24),\n",
    "            *_block(24, 48),\n",
    "            *_block(48, 48), # [..., 48, 8, 8]\n",
    "#             nn.AvgPool2d(kernel_size=8), # [..., 48, 1, 1]\n",
    "            nn.AvgPool2d(kernel_size=16), # [..., 48, 1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor, input_mask: Tensor, output_mask: Tensor):\n",
    "        out = torch.where(input_mask, x, torch.zeros_like(x))\n",
    "        out = self.layers(x)\n",
    "        return torch.where(output_mask, out, torch.zeros_like(out))\n",
    "\n",
    "# E_m = EncoderMaterial().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand((8, 3, 512, 512))\n",
    "# mask = torch.rand((8, 1, 512, 512)) > 0.5\n",
    "# # emb = torch.rand((8, 48, 8, 8))\n",
    "\n",
    "# m = EncoderMaterial()(x, mask)\n",
    "# m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeneratorReal()(emb, m).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# VGG architecter, used for the perceptual loss using a pretrained VGG network\n",
    "class VGG19(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super().__init__()\n",
    "        vgg_pretrained_features = torchvision.models.vgg19(pretrained=True).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(2, 7):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(7, 12):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(12, 21):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(21, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h_relu1 = self.slice1(X)\n",
    "        h_relu2 = self.slice2(h_relu1)\n",
    "        h_relu3 = self.slice3(h_relu2)\n",
    "        h_relu4 = self.slice4(h_relu3)\n",
    "        h_relu5 = self.slice5(h_relu4)\n",
    "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
    "        return out\n",
    "\n",
    "# Perceptual loss that uses a pretrained VGG network\n",
    "class VGGLoss(nn.Module):\n",
    "    transforms = transforms.Compose([\n",
    "        transforms.Normalize(mean=[0., 0., 0.], std=[1/0.5, 1/0.5, 1/0.5]),\n",
    "        transforms.Normalize(mean=[-0.5, -0.5, -0.5], std=[1, 1, 1]),\n",
    "        transforms.Resize((224, 224)), # better without centercrop?\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGLoss, self).__init__()\n",
    "        self.vgg = VGG19().to(device)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_resized = self.transforms(x)\n",
    "        y_resized = self.transforms(y)\n",
    "        \n",
    "        x_vgg, y_vgg = self.vgg(x_resized), self.vgg(y_resized)\n",
    "        loss = 0\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/stargan/models.py\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *discriminator_block(3, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1, bias=False),\n",
    "        )\n",
    "        self.layers2 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 1), # 512x512\n",
    "#             nn.Linear(256, 1),  # 256x256\n",
    "        )\n",
    "\n",
    "    def forward(self, img: Tensor) -> Tensor:\n",
    "        # combined = torch.cat((img_a, img_b), dim=1)\n",
    "        # out = img\n",
    "        # # print(\"Discriminator start\", out.shape)\n",
    "        #\n",
    "        # for layer in self.layers:\n",
    "        #     out = layer(out)\n",
    "        #     # print(layer, out.shape)\n",
    "\n",
    "        out = self.layers(img)\n",
    "        out = self.layers2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *discriminator_block(96, 192, normalization=False),\n",
    "            *discriminator_block(192, 192 * 2),\n",
    "            nn.Flatten(),\n",
    "#             nn.Linear(192 * 2 * 2 * 2, 1), # 256x256\n",
    "            nn.Linear(192 * 2 * 2 * 2 * 4, 1), # 512x512\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Options:\n",
    "    img_height = 512\n",
    "    img_width = 512\n",
    "#     learning_rate = 0.0001  # from paper\n",
    "    learning_rate = 0.00001\n",
    "    starting_epoch = 548\n",
    "    total_epochs = 700\n",
    "    batch_size = 8\n",
    "    cpu_count = 2\n",
    "\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    transform = transforms.Compose([\n",
    "#         transforms.Resize((opt.img_height, opt.img_width), transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    def __init__(self, mode=\"train\"):\n",
    "        self.files = []\n",
    "\n",
    "        for i in range(70):\n",
    "            for j in range(1000):\n",
    "                img_dir = f\"{i:02}000\"\n",
    "                img_name = f\"{i:02}{j:03}\"\n",
    "                if os.path.exists(f\"{FFHQ_DIR}/{img_dir}/{img_name}.jpg\"):\n",
    "                    if os.path.exists(f\"{RENDERED_ROOT}/{img_dir}_{img_name}_base.png\"):\n",
    "                        if os.path.exists(f\"{RENDERED_ROOT}/{img_dir}_{img_name}_structure.png\"):\n",
    "                            self.files.append((img_dir, img_name))\n",
    "\n",
    "        for i in range(30_000):\n",
    "            img_dir = \"celeba\"\n",
    "            img_name = str(i)\n",
    "            if os.path.exists(f\"{FFHQ_DIR}/{img_dir}/{img_name}.jpg\"):\n",
    "                if os.path.exists(f\"{STRUCTURE_ROOT}/{img_dir}/{img_name}.png\"):\n",
    "                    if os.path.exists(f\"{RENDERED_ROOT}/{img_dir}_{img_name}_base.png\"):\n",
    "                        if os.path.exists(f\"{RENDERED_ROOT}/{img_dir}_{img_name}_structure.png\"):\n",
    "                            self.files.append((img_dir, img_name))\n",
    "\n",
    "        print(f\"Read {len(self.files)} images\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_dir, img_name = self.files[index % len(self.files)]\n",
    "        \n",
    "        photo = Image.open(f\"{FFHQ_DIR}/{img_dir}/{img_name}.jpg\")\n",
    "        photo = photo.resize((opt.img_height, opt.img_width))\n",
    "        photo.putalpha(255)\n",
    "\n",
    "        render_base = Image.open(f\"{RENDERED_ROOT}/{img_dir}_{img_name}_base.png\")\n",
    "        render_base = render_base.resize((opt.img_height, opt.img_width))\n",
    "        render_struct = Image.open(f\"{RENDERED_ROOT}/{img_dir}_{img_name}_structure.png\")\n",
    "        render_struct = render_struct.resize((opt.img_height, opt.img_width))\n",
    "\n",
    "        render_base = Image.alpha_composite(photo, render_base).convert(\"RGB\")\n",
    "        render_struct = Image.alpha_composite(photo, render_struct).convert(\"RGB\")\n",
    "        \n",
    "        photo_structure = Image.open(f\"{STRUCTURE_ROOT}/{img_dir}/{img_name}.png\").convert(\"RGB\")\n",
    "        \n",
    "        mask_structure = Image.open(f\"{MASK_ROOT}/{img_dir}/{img_name}.png\")\n",
    "        mask_structure = mask_structure.resize((opt.img_height, opt.img_width))\n",
    "        mask_structure = (np.array(mask_structure) > 128)[:,:,:1] # only last channel\n",
    "\n",
    "#         render_mask = Image.open(f\"{RENDERED_ROOT}/{img_dir}_{img_name}_base.png\")\n",
    "#         render_mask = render_mask.resize((opt.img_height, opt.img_width))\n",
    "#         render_mask = (np.array(render_mask) > 128)[:,:,:1] # last channel\n",
    "        \n",
    "        return {\n",
    "            \"photo_base\": self.transform(photo.convert(\"RGB\")),\n",
    "            \"photo_struct\": self.transform(photo_structure),\n",
    "            \"mask\": torch.from_numpy(mask_structure).permute((2, 0, 1)),\n",
    "            \"render_base\": self.transform(render_base),\n",
    "            \"render_struct\": self.transform(render_struct),\n",
    "#             \"render_mask\": torch.from_numpy(render_mask.astype(\"float32\")).permute((2, 0, 1)),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25120 images\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(),\n",
    "    batch_size=opt.batch_size,\n",
    "    num_workers=opt.cpu_count,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    print(classname)\n",
    "    if hasattr(m, 'weight') and m.weight is not None and classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_loss(generated_scores: torch.Tensor,\n",
    "             real_scores: torch.Tensor,\n",
    "             for_generator: bool) -> torch.Tensor:\n",
    "    if for_generator:\n",
    "        return F.logsigmoid(-generated_scores).mean()\n",
    "\n",
    "    return -F.logsigmoid(real_scores).mean() - F.logsigmoid(-generated_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_loss_v2(generated_scores: tp.Optional[torch.Tensor],\n",
    "                real_scores: tp.Optional[torch.Tensor],\n",
    "                for_generator: bool) -> torch.Tensor:\n",
    "    loss = 0\n",
    "    \n",
    "    if generated_scores is not None:\n",
    "        loss += -F.logsigmoid(-generated_scores).mean()\n",
    "    if real_scores is not None:\n",
    "        loss += -F.logsigmoid(real_scores).mean()\n",
    "    \n",
    "    if for_generator:\n",
    "        return -loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_accuracy(generated_scores: torch.Tensor,\n",
    "                           real_scores: torch.Tensor) -> float:\n",
    "    return torch.cat((generated_scores < 0, real_scores > 0)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_imgs(x: Tensor):\n",
    "    # normalize: x -> (x - mu) / sigma\n",
    "    # denormalize: x -> x * sigma + mu\n",
    "    img = x.cpu().detach().permute((0, 2, 3, 1)).numpy()\n",
    "    img = (img * 0.5 + 0.5) * 255\n",
    "    return img.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid_to_writer(writer, x, label, iteration):\n",
    "    grid = torchvision.utils.make_grid(x, normalize=True, nrow=opt.batch_size)\n",
    "    writer.add_image(label, grid, iteration, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_param_count(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySuperNetwork(nn.Module):\n",
    "    lambda_vgg = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.E_f = nn.Sequential(\n",
    "            DownsampleBlock(),\n",
    "            *(ResBlk(96) for _ in range(4)),\n",
    "        )\n",
    "        self.E_shared = nn.Sequential(\n",
    "            ResBlk(96),\n",
    "            ResBlk(96),\n",
    "        )\n",
    "        self.G_f = GeneratorFake()\n",
    "        self.D_f = Discriminator()\n",
    "        \n",
    "        self.E_r = nn.Sequential(\n",
    "            DownsampleBlock(),\n",
    "            *(ResBlk(96) for _ in range(4)),\n",
    "        )\n",
    "        self.E_m = EncoderMaterial()\n",
    "        self.G_r = GeneratorReal(48)\n",
    "        self.D_r = Discriminator()\n",
    "        \n",
    "        self.D_embedding = EmbeddingDiscriminator()\n",
    "        \n",
    "        self.vgg_loss = VGGLoss()\n",
    "        \n",
    "        print(\"Trainable params:\")\n",
    "        print(\"E_f:\", trainable_param_count(self.E_f))\n",
    "        print(\"E_shared:\", trainable_param_count(self.E_shared))\n",
    "        print(\"E_r:\", trainable_param_count(self.E_r))\n",
    "        print(\"D_embedding:\", trainable_param_count(self.D_embedding))\n",
    "        \n",
    "        print(\"G_f:\", trainable_param_count(self.G_f))\n",
    "        print(\"D_f:\", trainable_param_count(self.D_f))\n",
    "        print(\"E_m:\", trainable_param_count(self.E_m))\n",
    "        print(\"G_r:\", trainable_param_count(self.G_r))\n",
    "        print(\"D_r:\", trainable_param_count(self.D_r))\n",
    "\n",
    "\n",
    "    def create_optimizers(self, opt):\n",
    "        gan_params = list(self.E_f.parameters())\n",
    "        gan_params += list(self.E_shared.parameters())\n",
    "        gan_params += list(self.G_f.parameters())\n",
    "        \n",
    "        gan_params += list(self.E_r.parameters())\n",
    "        gan_params += list(self.E_m.parameters())\n",
    "        gan_params += list(self.G_r.parameters())\n",
    "\n",
    "        disc_params = list(self.D_f.parameters())\n",
    "        disc_params += list(self.D_r.parameters())\n",
    "        disc_params += list(self.D_embedding.parameters())\n",
    "\n",
    "        params_F = list(self.E_f.parameters())\n",
    "        params_F += list(self.E_shared.parameters())\n",
    "        params_F += list(self.G_f.parameters())\n",
    "        \n",
    "        params_R = list(self.E_r.parameters())\n",
    "        params_R += list(self.E_shared.parameters())\n",
    "        params_R += list(self.E_m.parameters())\n",
    "        params_R += list(self.G_r.parameters())\n",
    "        \n",
    "        gan_optimizer = torch.optim.Adam(gan_params, lr=opt.learning_rate)\n",
    "        disc_optimizer = torch.optim.Adam(disc_params, lr=opt.learning_rate)\n",
    "        disc_emb_optimizer = torch.optim.Adam(self.D_embedding.parameters(), lr=opt.learning_rate)\n",
    "        \n",
    "        F_optimizer = torch.optim.Adam(params_F, lr=opt.learning_rate)\n",
    "        R_optimizer = torch.optim.Adam(params_R, lr=opt.learning_rate)\n",
    "\n",
    "        return gan_optimizer, disc_optimizer, disc_emb_optimizer, F_optimizer, R_optimizer\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.E_f.apply(weights_init_normal)\n",
    "        self.E_shared.apply(weights_init_normal)\n",
    "        self.G_f.apply(weights_init_normal)\n",
    "        self.D_f.apply(weights_init_normal)\n",
    "        \n",
    "        self.E_r.apply(weights_init_normal)\n",
    "        self.E_m.apply(weights_init_normal)\n",
    "        self.G_r.apply(weights_init_normal)\n",
    "        self.D_r.apply(weights_init_normal)\n",
    "    \n",
    "    def _fake_branch(self, fake_input):\n",
    "        embedding_fake = self.E_shared(self.E_f(fake_input))\n",
    "        generated_fake = self.G_f(embedding_fake)\n",
    "        return generated_fake, embedding_fake\n",
    "    \n",
    "    def fake_branch_generator(self, fake_input, fake_truths):\n",
    "        generated_fake, embedding_fake = self._fake_branch(fake_input)\n",
    "\n",
    "        generated_fake_scores = self.D_f(generated_fake)\n",
    "        truth_fake_scores = self.D_f(fake_truths)\n",
    "        \n",
    "        embedding_fake_scores = self.D_embedding(embedding_fake)\n",
    "\n",
    "        losses = {\n",
    "            \"generator_F_loss\": gan_loss(generated_fake_scores, None, for_generator=True),\n",
    "            \"VGG_F_loss\": self.vgg_loss(generated_fake, fake_truths) * self.lambda_vgg,\n",
    "            \"D_EMB_F_loss\": -gan_loss(embedding_fake_scores, Tensor([]), for_generator=False),\n",
    "\n",
    "        }\n",
    "        accuracies = {\n",
    "            \"D_F_accuracy\": discriminator_accuracy(generated_fake_scores.detach(),\n",
    "                                                   truth_fake_scores.detach()),\n",
    "            \"D_EMB_F_accuracy\": discriminator_accuracy(embedding_fake_scores.detach(),\n",
    "                                                       Tensor([])),\n",
    "        }\n",
    "        return generated_fake, losses, accuracies\n",
    "\n",
    "#     def fake_branch_discriminator(self, fake_input, fake_truths):\n",
    "#         with torch.no_grad():\n",
    "#             generated_fake, embedding_fake = self._fake_branch(fake_input, fake_truths)\n",
    "#             generated_fake = generated_fake.detach()\n",
    "#             embedding_fake = embedding_fake.detach()\n",
    "\n",
    "#         generated_fake_scores = self.D_f(generated_fake)\n",
    "#         truth_fake_scores = self.D_f(fake_truths)\n",
    "\n",
    "#         embedding_fake_scores = self.D_embedding(embedding_fake)\n",
    "\n",
    "#         losses = {\n",
    "#             \"D_F_loss\": gan_loss(generated_fake_scores, truth_fake_scores, for_generator=False),\n",
    "#             \"D_EMB_F_loss\": gan_loss_v2(embedding_fake_scores, None, for_generator=False),\n",
    "#         }\n",
    "#         accuracies = {\n",
    "#             \"D_F_accuracy\": discriminator_accuracy(generated_fake_scores.detach(),\n",
    "#                                                    truth_fake_scores.detach()),\n",
    "#             \"D_EMB_accuracy\": discriminator_accuracy(embedding_fake_scores.detach(),\n",
    "#                                                      Tensor([])),\n",
    "#         }\n",
    "\n",
    "#         return generated_fake, generated_real, losses, accuracies\n",
    "\n",
    "    \n",
    "    def _real_branch(self, real_input, real_truth, real_masks):\n",
    "        embedding_real = self.E_shared(self.E_r(real_input))\n",
    "        segmentation_mask = self.E_m(real_truth, real_masks, real_masks)\n",
    "        generated_real = self.G_r(embedding_real, segmentation_mask)\n",
    "        return generated_real, embedding_real\n",
    "    \n",
    "\n",
    "    def real_branch_generator(self, real_input, real_truth, real_masks):\n",
    "        generated_real, embedding_real = self._real_branch(real_input, real_truth, real_masks)\n",
    "\n",
    "        generated_real_scores = self.D_r(generated_real)\n",
    "        truth_real_scores = self.D_r(real_truth)\n",
    "\n",
    "#         embedding_real_scores = self.D_embedding(embedding_real)\n",
    "\n",
    "        losses = {\n",
    "            \"generator_R_loss\": gan_loss(generated_real_scores, None, for_generator=True),\n",
    "            \"VGG_R_loss\": self.vgg_loss(generated_real, real_truth) * self.lambda_vgg,\n",
    "#             \"D_EMB_R_loss\": -gan_loss(Tensor([]), embedding_real_scores, for_generator=False),\n",
    "\n",
    "        }\n",
    "        accuracies = {\n",
    "            \"D_R_accuracy\": discriminator_accuracy(generated_real_scores.detach(),\n",
    "                                                   truth_real_scores.detach()),\n",
    "#             \"D_EMB_R_accuracy\": discriminator_accuracy(Tensor([]),\n",
    "#                                                        embedding_real_scores.detach()),\n",
    "        }\n",
    "        return generated_real, losses, accuracies\n",
    "\n",
    "    \n",
    "    def real_branch_discriminator(self, real_input, real_truth, real_masks):\n",
    "        with torch.no_grad():\n",
    "            generated_real, _ = self._real_branch(real_input, real_truth, real_masks)\n",
    "            generated_real = generated_real.detach()\n",
    "\n",
    "        generated_real_scores = self.D_r(generated_real)\n",
    "        truth_real_scores = self.D_r(real_truth)\n",
    "\n",
    "#         embedding_fake_scores = self.D_embedding(embedding_fake)\n",
    "#         embedding_real_scores = self.D_embedding(embedding_real)\n",
    "\n",
    "        losses = {\n",
    "#             \"D_F_loss\": gan_loss(generated_fake_scores, truth_fake_scores, for_generator=False),\n",
    "            \"D_R_loss\": gan_loss(generated_real_scores, truth_real_scores, for_generator=False),\n",
    "#             \"D_EMB_loss\": gan_loss(embedding_fake_scores, embedding_real_scores, for_generator=False),\n",
    "        }\n",
    "        accuracies = {\n",
    "#             \"D_F_accuracy\": discriminator_accuracy(generated_fake_scores.detach(),\n",
    "#                                                    truth_fake_scores.detach()),\n",
    "            \"D_R_accuracy\": discriminator_accuracy(generated_real_scores.detach(),\n",
    "                                                   truth_real_scores.detach()),\n",
    "#             \"D_EMB_accuracy\": discriminator_accuracy(embedding_fake_scores.detach(),\n",
    "#                                                      embedding_real_scores.detach()),\n",
    "        }\n",
    "\n",
    "        return generated_real, losses, accuracies\n",
    "\n",
    "    \n",
    "    def _both_branches(self, fake_input, fake_truths, real_input, real_truth, real_masks):\n",
    "        embedding_fake = self.E_shared(self.E_f(fake_input))\n",
    "        generated_fake = self.G_f(embedding_fake)\n",
    "        \n",
    "        embedding_real = self.E_shared(self.E_r(real_input))\n",
    "        segmentation_mask = self.E_m(real_truth, real_masks, real_masks)\n",
    "        generated_real = self.G_r(embedding_real, segmentation_mask)\n",
    "        \n",
    "        return generated_fake, embedding_fake, generated_real, embedding_real\n",
    "    \n",
    "#     def both_generators(self, fake_input, fake_truths, real_input, real_truth, real_masks):\n",
    "#         generated_fake, embedding_fake, generated_real, embedding_real = self._both_branches(\n",
    "#             fake_input, fake_truths, real_input, real_truth, real_masks)\n",
    "        \n",
    "#         generated_fake_scores = self.D_f(generated_fake)\n",
    "#         truth_fake_scores = self.D_f(fake_truths)\n",
    "        \n",
    "#         generated_real_scores = self.D_r(generated_real)\n",
    "#         truth_real_scores = self.D_r(real_truth)\n",
    "\n",
    "#         embedding_fake_scores = self.D_embedding(embedding_fake)\n",
    "#         embedding_real_scores = self.D_embedding(embedding_real)\n",
    "\n",
    "#         losses = {\n",
    "#             \"generator_F_loss\": gan_loss(generated_fake_scores, None, for_generator=True),\n",
    "#             \"VGG_F_loss\": self.vgg_loss(generated_fake, fake_truths) * self.lambda_vgg,\n",
    "            \n",
    "#             \"generator_R_loss\": gan_loss(generated_real_scores, None, for_generator=True),\n",
    "#             \"VGG_R_loss\": self.vgg_loss(generated_real, real_truth) * self.lambda_vgg,\n",
    "            \n",
    "#             # distinguishing both branches therefore False to compute all scores\n",
    "#             \"-D_EMB_loss\": -gan_loss(embedding_fake_scores, embedding_real_scores, for_generator=False),\n",
    "#         }\n",
    "#         accuracies = {\n",
    "#             \"D_F_accuracy\": discriminator_accuracy(generated_fake_scores.detach(),\n",
    "#                                                    truth_fake_scores.detach()),\n",
    "#             \"D_R_accuracy\": discriminator_accuracy(generated_real_scores.detach(),\n",
    "#                                                    truth_real_scores.detach()),\n",
    "#             \"D_EMB_accuracy\": discriminator_accuracy(embedding_fake_scores.detach(),\n",
    "#                                                      embedding_real_scores.detach()),\n",
    "#         }\n",
    "#         return generated_fake, generated_real, losses, accuracies\n",
    "    \n",
    "    def both_discriminators(self, fake_input, fake_truths, real_input, real_truth, real_masks):\n",
    "        with torch.no_grad():\n",
    "            outputs = self._both_branches(\n",
    "                fake_input,\n",
    "                fake_truths,\n",
    "                real_input,\n",
    "                real_truth,\n",
    "                real_masks\n",
    "            )\n",
    "            generated_fake = outputs[0].detach()\n",
    "            embedding_fake = outputs[1].detach()\n",
    "            generated_real = outputs[2].detach()\n",
    "            embedding_real = outputs[3].detach()\n",
    "\n",
    "        generated_fake_scores = self.D_f(generated_fake)\n",
    "        truth_fake_scores = self.D_f(fake_truths)\n",
    "        \n",
    "        generated_real_scores = self.D_r(generated_real)\n",
    "        truth_real_scores = self.D_r(real_truth)\n",
    "\n",
    "        embedding_fake_scores = self.D_embedding(embedding_fake)\n",
    "        embedding_real_scores = self.D_embedding(embedding_real)\n",
    "\n",
    "        losses = {\n",
    "            \"D_F_loss\": gan_loss(generated_fake_scores, truth_fake_scores, for_generator=False),\n",
    "            \"D_R_loss\": gan_loss(generated_real_scores, truth_real_scores, for_generator=False),\n",
    "            \"D_EMB_loss\": gan_loss(embedding_fake_scores, embedding_real_scores, for_generator=False),\n",
    "        }\n",
    "        accuracies = {\n",
    "            \"D_F_accuracy\": discriminator_accuracy(generated_fake_scores.detach(),\n",
    "                                                   truth_fake_scores.detach()),\n",
    "            \"D_R_accuracy\": discriminator_accuracy(generated_real_scores.detach(),\n",
    "                                                   truth_real_scores.detach()),\n",
    "            \"D_EMB_accuracy\": discriminator_accuracy(embedding_fake_scores.detach(),\n",
    "                                                     embedding_real_scores.detach()),\n",
    "        }\n",
    "\n",
    "        return generated_fake, generated_real, losses, accuracies\n",
    "\n",
    "    def cheeky_embedding_discriminator(self, fake_input, real_input):\n",
    "        with torch.no_grad():\n",
    "            embedding_fake = self.E_shared(self.E_f(fake_input))\n",
    "            embedding_fake = embedding_fake.detach()\n",
    "            \n",
    "            embedding_real = self.E_shared(self.E_r(fake_input))\n",
    "            embedding_real = embedding_real.detach()\n",
    "        \n",
    "        emb_fake_scores = self.D_embedding(embedding_fake)\n",
    "        emb_real_scores = self.D_embedding(embedding_real)\n",
    "\n",
    "        losses = {\n",
    "            \"D_EMB_loss\": gan_loss(emb_fake_scores, emb_real_scores, for_generator=False),\n",
    "        }\n",
    "        accuracies = {\n",
    "            \"D_EMB_accuracy\": discriminator_accuracy(emb_fake_scores.detach(),\n",
    "                                                     emb_real_scores.detach()),\n",
    "        }\n",
    "        return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MySuperNetwork2(nn.Module):\n",
    "#     lambda_vgg = 10\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.E_f = nn.Sequential(\n",
    "#             DownsampleBlock(),\n",
    "#             *(ResBlk(96) for _ in range(4)),\n",
    "#         )\n",
    "#         self.E_shared = nn.Sequential(\n",
    "#             ResBlk(96),\n",
    "#             ResBlk(96),\n",
    "#         )\n",
    "#         self.G_f = GeneratorReal(1)\n",
    "#         self.D_f = Discriminator()\n",
    "        \n",
    "#         self.E_r = nn.Sequential(\n",
    "#             DownsampleBlock(),\n",
    "#             *(ResBlk(96) for _ in range(4)),\n",
    "#         )\n",
    "#         self.E_m = EncoderMaterial()\n",
    "#         self.G_r = GeneratorReal(48)\n",
    "#         self.D_r = Discriminator()\n",
    "        \n",
    "#         self.D_embedding = EmbeddingDiscriminator()\n",
    "        \n",
    "#         self.vgg_loss = VGGLoss()\n",
    "        \n",
    "#         print(\"Trainable params:\")\n",
    "#         print(\"E_f:\", trainable_param_count(self.E_f))\n",
    "#         print(\"E_shared:\", trainable_param_count(self.E_shared))\n",
    "#         print(\"E_r:\", trainable_param_count(self.E_r))\n",
    "#         print(\"D_embedding:\", trainable_param_count(self.D_embedding))\n",
    "        \n",
    "#         print(\"G_f:\", trainable_param_count(self.G_f))\n",
    "#         print(\"D_f:\", trainable_param_count(self.D_f))\n",
    "#         print(\"E_m:\", trainable_param_count(self.E_m))\n",
    "#         print(\"G_r:\", trainable_param_count(self.G_r))\n",
    "#         print(\"D_r:\", trainable_param_count(self.D_r))\n",
    "\n",
    "#     def create_optimizers(self, opt):\n",
    "#         gan_params = list(self.E_f.parameters())\n",
    "#         gan_params += list(self.E_shared.parameters())\n",
    "#         gan_params += list(self.G_f.parameters())\n",
    "        \n",
    "#         gan_params += list(self.E_r.parameters())\n",
    "#         gan_params += list(self.E_m.parameters())\n",
    "#         gan_params += list(self.G_r.parameters())\n",
    "\n",
    "#         disc_params = list(self.D_f.parameters())\n",
    "#         disc_params += list(self.D_r.parameters())\n",
    "#         disc_params += list(self.D_embedding.parameters())\n",
    "\n",
    "#         gan_optimizer = torch.optim.Adam(gan_params, lr=opt.learning_rate)\n",
    "#         disc_optimizer = torch.optim.Adam(disc_params, lr=opt.learning_rate)\n",
    "#         disc_emb_optimizer = torch.optim.Adam(self.D_embedding.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "#         return gan_optimizer, disc_optimizer, disc_emb_optimizer\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         self.E_f.apply(weights_init_normal)\n",
    "#         self.E_shared.apply(weights_init_normal)\n",
    "#         self.G_f.apply(weights_init_normal)\n",
    "#         self.D_f.apply(weights_init_normal)\n",
    "        \n",
    "#         self.E_r.apply(weights_init_normal)\n",
    "#         self.E_m.apply(weights_init_normal)\n",
    "#         self.G_r.apply(weights_init_normal)\n",
    "#         self.D_r.apply(weights_init_normal)\n",
    "    \n",
    "# #     def _fake_branch(self, input_images):\n",
    "# #         embedding = self.E_shared(self.E_f(input_images))\n",
    "# #         generated = self.G_f(embedding)\n",
    "# #         return generated\n",
    "    \n",
    "# #     def _real_branch(self, input_images, reference_images, masks):\n",
    "# #         embedding = self.E_shared(self.E_r(input_images))\n",
    "# #         material_embedding = self.E_m(reference_images, masks)\n",
    "# #         generated = self.G_r(embedding, material_embedding)\n",
    "# #         return generated\n",
    "    \n",
    "# #     def fake_branch_generator(self, input_images, ground_truth):\n",
    "# #         generated = self._fake_branch(input_images)\n",
    "# #         generated_scores = self.D_f(generated)\n",
    "\n",
    "# #         losses = dict()\n",
    "# #         losses[\"branch_F_generator_loss\"] = gan_loss(generated_scores, None, for_generator=True)\n",
    "# #         losses[\"VGG_F_loss\"] = self.vgg_loss(generated, ground_truth) * self.lambda_vgg\n",
    "\n",
    "# #         return generated, losses\n",
    "\n",
    "# #     def fake_branch_discriminator(self, input_images, ground_truth):\n",
    "# #         with torch.no_grad():\n",
    "# #             generated = self._fake_branch(input_images)\n",
    "# #             generated = generated.detach()\n",
    "# #             generated.requires_grad_() # eh?\n",
    "\n",
    "# #         generated_scores = self.D_f(generated)\n",
    "# #         truth_scores = self.D_f(ground_truth)\n",
    "\n",
    "# #         losses = {\n",
    "# #             \"D_F_loss\": gan_loss(generated_scores, truth_scores, for_generator=False),\n",
    "# #             \"D_F_accuracy\": discriminator_accuracy(generated_scores.detach(), truth_scores.detach()), # eh? detach?\n",
    "# #         }\n",
    "# #         return generated, losses\n",
    "\n",
    "# #     def real_branch_generator(self, input_images, ground_truth, masks):\n",
    "# #         generated = self._real_branch(input_images, ground_truth, masks)\n",
    "# #         generated_scores = self.D_r(generated)\n",
    "        \n",
    "# #         losses = dict()\n",
    "# #         losses[\"branch_R_generator_loss\"] = gan_loss(generated_scores, None, for_generator=True)\n",
    "# #         losses[\"VGG_R_loss\"] = self.vgg_loss(generated, ground_truth) * self.lambda_vgg\n",
    "\n",
    "# #         return generated, losses\n",
    "    \n",
    "# #     def real_branch_discriminator(self, input_images, ground_truth, masks):\n",
    "# #         with torch.no_grad():\n",
    "# #             generated = self._real_branch(input_images, ground_truth, masks)\n",
    "# #             generated = generated.detach()\n",
    "# #             generated.requires_grad_() # eh?\n",
    "\n",
    "# #         generated_scores = self.D_r(generated)\n",
    "# #         truth_scores = self.D_r(ground_truth)\n",
    "\n",
    "# #         losses = {\n",
    "# #             \"D_R_loss\": gan_loss(generated_scores, truth_scores, for_generator=False),\n",
    "# #             \"D_R_accuracy\": discriminator_accuracy(generated_scores.detach(), truth_scores.detach()), # eh? detach?\n",
    "# #         }\n",
    "# #         return generated, losses\n",
    "    \n",
    "#     def _both_branches(self, fake_input, fake_truths, fake_masks, real_input, real_truth, real_masks):\n",
    "#         embedding_fake = self.E_shared(self.E_f(fake_input))\n",
    "#         generated_fake = self.G_f(embedding_fake, fake_masks)\n",
    "        \n",
    "#         embedding_real = self.E_shared(self.E_r(real_input))\n",
    "#         segmentation_mask = self.E_m(real_truth, real_masks)\n",
    "#         generated_real = self.G_r(embedding_real, segmentation_mask)\n",
    "        \n",
    "#         return generated_fake, embedding_fake, generated_real, embedding_real\n",
    "    \n",
    "#     def both_generators(self, fake_input, fake_truths, fake_masks, real_input, real_truth, real_masks):\n",
    "#         generated_fake, embedding_fake, generated_real, embedding_real = self._both_branches(\n",
    "#             fake_input, fake_truths, fake_masks, real_input, real_truth, real_masks)\n",
    "        \n",
    "#         generated_fake_scores = self.D_f(generated_fake)\n",
    "#         truth_fake_scores = self.D_f(fake_truths)\n",
    "        \n",
    "#         generated_real_scores = self.D_r(generated_real)\n",
    "#         truth_real_scores = self.D_r(real_truth)\n",
    "\n",
    "#         embedding_fake_scores = self.D_embedding(embedding_fake)\n",
    "#         embedding_real_scores = self.D_embedding(embedding_real)\n",
    "\n",
    "#         losses = {\n",
    "#             \"generator_F_loss\": gan_loss(generated_fake_scores, None, for_generator=True),\n",
    "#             \"VGG_F_loss\": self.vgg_loss(generated_fake, fake_truths) * self.lambda_vgg,\n",
    "            \n",
    "#             \"generator_R_loss\": gan_loss(generated_real_scores, None, for_generator=True),\n",
    "#             \"VGG_R_loss\": self.vgg_loss(generated_real, real_truth) * self.lambda_vgg,\n",
    "            \n",
    "#             # distinguishing both branches therefore False to compute all scores\n",
    "#             \"-D_EMB_loss\": -gan_loss(embedding_fake_scores, embedding_real_scores, for_generator=False),\n",
    "#         }\n",
    "#         accuracies = {\n",
    "#             \"D_F_accuracy\": discriminator_accuracy(generated_fake_scores.detach(),\n",
    "#                                                    truth_fake_scores.detach()),\n",
    "#             \"D_R_accuracy\": discriminator_accuracy(generated_real_scores.detach(),\n",
    "#                                                    truth_real_scores.detach()),\n",
    "#             \"D_EMB_accuracy\": discriminator_accuracy(embedding_fake_scores.detach(),\n",
    "#                                                      embedding_real_scores.detach()),\n",
    "#         }\n",
    "#         return generated_fake, generated_real, losses, accuracies\n",
    "    \n",
    "#     def both_discriminators(self, fake_input, fake_truths, fake_masks, real_input, real_truth, real_masks):\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self._both_branches(\n",
    "#                 fake_input,\n",
    "#                 fake_truths,\n",
    "#                 fake_masks,\n",
    "#                 real_input,\n",
    "#                 real_truth,\n",
    "#                 real_masks\n",
    "#             )\n",
    "#             generated_fake = outputs[0].detach()\n",
    "#             embedding_fake = outputs[1].detach()\n",
    "#             generated_real = outputs[2].detach()\n",
    "#             embedding_real = outputs[3].detach()\n",
    "\n",
    "#         generated_fake_scores = self.D_f(generated_fake)\n",
    "#         truth_fake_scores = self.D_f(fake_truths)\n",
    "        \n",
    "#         generated_real_scores = self.D_r(generated_real)\n",
    "#         truth_real_scores = self.D_r(real_truth)\n",
    "\n",
    "#         embedding_fake_scores = self.D_embedding(embedding_fake)\n",
    "#         embedding_real_scores = self.D_embedding(embedding_real)\n",
    "\n",
    "#         losses = {\n",
    "#             \"D_F_loss\": gan_loss(generated_fake_scores, truth_fake_scores, for_generator=False),\n",
    "#             \"D_R_loss\": gan_loss(generated_real_scores, truth_real_scores, for_generator=False),\n",
    "#             \"D_EMB_loss\": gan_loss(embedding_fake_scores, embedding_real_scores, for_generator=False),\n",
    "#         }\n",
    "#         accuracies = {\n",
    "#             \"D_F_accuracy\": discriminator_accuracy(generated_fake_scores.detach(),\n",
    "#                                                    truth_fake_scores.detach()),\n",
    "#             \"D_R_accuracy\": discriminator_accuracy(generated_real_scores.detach(),\n",
    "#                                                    truth_real_scores.detach()),\n",
    "#             \"D_EMB_accuracy\": discriminator_accuracy(embedding_fake_scores.detach(),\n",
    "#                                                      embedding_real_scores.detach()),\n",
    "#         }\n",
    "\n",
    "#         return generated_fake, generated_real, losses, accuracies\n",
    "\n",
    "#     def cheeky_embedding_discriminator(self, fake_input, real_input):\n",
    "#         with torch.no_grad():\n",
    "#             embedding_fake = self.E_shared(self.E_f(fake_input))\n",
    "#             embedding_fake = embedding_fake.detach()\n",
    "            \n",
    "#             embedding_real = self.E_shared(self.E_r(fake_input))\n",
    "#             embedding_real = embedding_real.detach()\n",
    "        \n",
    "#         emb_fake_scores = self.D_embedding(embedding_fake)\n",
    "#         emb_real_scores = self.D_embedding(embedding_real)\n",
    "\n",
    "#         losses = {\n",
    "#             \"D_EMB_loss\": gan_loss(emb_fake_scores, emb_real_scores, for_generator=False),\n",
    "#         }\n",
    "#         accuracies = {\n",
    "#             \"D_EMB_accuracy\": discriminator_accuracy(emb_fake_scores.detach(),\n",
    "#                                                      emb_real_scores.detach()),\n",
    "#         }\n",
    "#         return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params:\n",
      "E_f: 721656\n",
      "E_shared: 332928\n",
      "E_r: 721656\n",
      "D_embedding: 1481281\n",
      "G_f: 388533\n",
      "D_f: 2765761\n",
      "E_m: 34920\n",
      "G_r: 2325849\n",
      "D_r: 2765761\n"
     ]
    }
   ],
   "source": [
    "if opt.starting_epoch == 0:\n",
    "    network = MySuperNetwork().to(device)\n",
    "    network.init_weights()\n",
    "    _, disc_optimizer, disc_emb_optimizer, F_optimizer, R_optimizer = network.create_optimizers(opt)\n",
    "    \n",
    "    log_iter = 0\n",
    "else:\n",
    "    network = MySuperNetwork().to(device)\n",
    "    _, disc_optimizer, disc_emb_optimizer, F_optimizer, R_optimizer = network.create_optimizers(opt)\n",
    "    \n",
    "    checkpoint = torch.load(\"/home/andresokol/weights/v5_fix_HD_96_latent_547.pt\")\n",
    "\n",
    "    network.load_state_dict(checkpoint['network_state_dict'])\n",
    "    network = network.to(device)\n",
    "    \n",
    "    log_iter = checkpoint['log_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fractions\n",
    "\n",
    "discriminator_freq = fractions.Fraction(1)\n",
    "discriminator_embedding_freq = fractions.Fraction(1)\n",
    "\n",
    "max_discriminator_freq = fractions.Fraction(64)\n",
    "min_discriminator_freq = fractions.Fraction(1, 64)\n",
    "\n",
    "def should_run_discriminator(i: int, discriminator_freq: fractions.Fraction):\n",
    "    if discriminator_freq > 1:\n",
    "        return i % discriminator_freq.numerator != 0\n",
    "\n",
    "    return i % (discriminator_freq.denominator * 2) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d61ed10fab141f585435e1515adb838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 504:   0%|          | 0/3140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "# https://github.com/hse-ds/iad-applied-ds/blob/master/2022/seminars/sem03/sem03-gan-task.ipynb\n",
    "# https://colab.research.google.com/github/yandexdataschool/mlhep2019/blob/master/notebooks/day-6/06_GAN_faces_solution.ipynb#scrollTo=-Z7YAPGxp2Xl\n",
    "\n",
    "losses_f = {}\n",
    "losses_d_f = {}\n",
    "losses_r = {}\n",
    "losses_d_r = {}\n",
    "\n",
    "run_fake_gen = True\n",
    "min_accuracy = 0\n",
    "losses, accuracies = {}, {\"D_EMB_accuracy\": Tensor(1),\n",
    "                          \"D_F_accuracy\": Tensor(1),\n",
    "                          \"D_R_accuracy\": Tensor(1),}\n",
    "\n",
    "for epoch in range(opt.starting_epoch, opt.total_epochs):\n",
    "    with tqdm.auto.tqdm(total=len(dataloader), unit=\"batch\", desc=f\"Epoch {epoch}\") as pbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            render_base = batch[\"render_base\"]\n",
    "            render_struct = batch[\"render_struct\"]\n",
    "\n",
    "            photo_base = batch[\"photo_base\"]\n",
    "            photo_struct = batch[\"photo_struct\"]\n",
    "            masks = batch[\"mask\"]\n",
    "\n",
    "            if should_run_discriminator(i + 1, discriminator_embedding_freq):\n",
    "                disc_emb_optimizer.zero_grad()\n",
    "                losses, accuracies = network.cheeky_embedding_discriminator(\n",
    "                    render_struct.to(device),\n",
    "                    photo_struct.to(device),\n",
    "                )\n",
    "                losses[\"D_EMB_loss\"].backward()\n",
    "                disc_emb_optimizer.step()\n",
    "                \n",
    "                if accuracies[\"D_EMB_accuracy\"] < 0.5 and discriminator_embedding_freq < max_discriminator_freq:\n",
    "                    discriminator_embedding_freq *= 2\n",
    "                elif accuracies[\"D_EMB_accuracy\"] > 0.8 and discriminator_embedding_freq > min_discriminator_freq:\n",
    "                    discriminator_embedding_freq /= 2\n",
    "\n",
    "            elif should_run_discriminator(i, discriminator_freq):\n",
    "                disc_optimizer.zero_grad()\n",
    "\n",
    "                generated_f, generated_r, losses, accuracies = network.both_discriminators(\n",
    "                    render_struct[::2].to(device),\n",
    "                    render_base[::2].to(device),\n",
    "                    photo_struct[::2].to(device),\n",
    "                    photo_base[::2].to(device),\n",
    "                    masks[::2].to(device),\n",
    "                )\n",
    "\n",
    "                discriminator_loss = sum(losses.values())\n",
    "                discriminator_loss.backward()\n",
    "\n",
    "                disc_optimizer.step()\n",
    "\n",
    "                # dup fix me?\n",
    "                min_accuracy = min(accuracies[\"D_F_accuracy\"], accuracies[\"D_R_accuracy\"]).item()\n",
    "                if min_accuracy < 0.8 and discriminator_freq < max_discriminator_freq:\n",
    "                    discriminator_freq *= 2\n",
    "                elif min_accuracy > 0.9 and discriminator_freq > min_discriminator_freq:\n",
    "                    discriminator_freq /= 2\n",
    "\n",
    "            else:\n",
    "                if run_fake_gen:\n",
    "                    F_optimizer.zero_grad()\n",
    "                    generated_f, losses_f, accuracies_f = network.fake_branch_generator(\n",
    "                        render_struct.to(device),\n",
    "                        render_base.to(device),\n",
    "                    )\n",
    "                    loss = sum(losses_f.values())\n",
    "                    loss.backward()\n",
    "                    F_optimizer.step()\n",
    "\n",
    "                    losses = {**losses, **losses_f}\n",
    "                    accuracies = {**accuracies, **accuracies_f, \"D_EMB_accuracy\": accuracies_f[\"D_EMB_F_accuracy\"]}\n",
    "                else:\n",
    "                    R_optimizer.zero_grad()\n",
    "                    generated_r, losses_r, accuracies_r = network.real_branch_generator(\n",
    "                        photo_struct.to(device),\n",
    "                        photo_base.to(device),\n",
    "                        masks.to(device),\n",
    "                    )\n",
    "                    loss = sum(losses_r.values())\n",
    "                    loss.backward()\n",
    "                    R_optimizer.step()\n",
    "                    \n",
    "                    losses = {**losses, **losses_r}\n",
    "                    accuracies = {**accuracies, **accuracies_r, \"D_EMB_accuracy\": accuracies_r[\"D_EMB_R_accuracy\"]}\n",
    "\n",
    "#                 print(accuracies)\n",
    "                min_accuracy = min(accuracies.get(\"D_F_accuracy\", Tensor(1)),\n",
    "                                   accuracies.get(\"D_R_accuracy\", Tensor(1))).item()\n",
    "                if min_accuracy < 0.8 and discriminator_freq < max_discriminator_freq:\n",
    "                    discriminator_freq *= 2\n",
    "                if accuracies[\"D_EMB_accuracy\"] < 0.5 and discriminator_embedding_freq < max_discriminator_freq:\n",
    "                    discriminator_embedding_freq *= 2\n",
    "                \n",
    "                if i % 50 == 0:\n",
    "                    image_grid_to_writer(\n",
    "                        writer,\n",
    "                        torch.cat([generated_f.cpu(), render_base])\n",
    "                        if run_fake_gen else \n",
    "                        torch.cat([generated_r.cpu(), photo_base]),\n",
    "                        \"images_f\" if run_fake_gen else \"images_r\",\n",
    "                        log_iter,\n",
    "                    )\n",
    "                run_fake_gen = not run_fake_gen\n",
    "\n",
    "            for d in [losses, accuracies]:\n",
    "                for key, value in d.items():\n",
    "                    writer.add_scalar(key, value.item(), log_iter)\n",
    "            writer.add_scalar(\"D_EMB_frequency\", float(discriminator_embedding_freq), log_iter)\n",
    "\n",
    "            pbar.set_postfix(discriminator_freq=str(discriminator_freq),\n",
    "                             min_accuracy=min_accuracy,\n",
    "                             d_emb_freq=str(discriminator_embedding_freq),\n",
    "                             d_emb_accuracy=accuracies[\"D_EMB_accuracy\"].item(),\n",
    "                            )\n",
    "            pbar.update(1)\n",
    "            log_iter += 1\n",
    "\n",
    "    if epoch % 2 == 1:\n",
    "        torch.save({\n",
    "            \"network_state_dict\": network.state_dict(),\n",
    "#             \"gan_optimizer_state_dict\": gan_optimizer.state_dict(),\n",
    "#             \"disc_optimizer\": disc_optimizer.state_dict(),\n",
    "#             \"disc_emb_optimizer\": disc_emb_optimizer.state_dict(),\n",
    "            \"log_iter\": log_iter,\n",
    "        }, f\"/home/andresokol/weights/v5_fix_HD_96_latent_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a3b6b018c24f008cfd3177e1dc9066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 548:   0%|          | 0/3140 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "# https://github.com/hse-ds/iad-applied-ds/blob/master/2022/seminars/sem03/sem03-gan-task.ipynb\n",
    "# https://colab.research.google.com/github/yandexdataschool/mlhep2019/blob/master/notebooks/day-6/06_GAN_faces_solution.ipynb#scrollTo=-Z7YAPGxp2Xl\n",
    "\n",
    "losses_f = {}\n",
    "losses_d_f = {}\n",
    "losses_r = {}\n",
    "losses_d_r = {}\n",
    "\n",
    "min_accuracy = 0\n",
    "losses, accuracies = {}, {\"D_EMB_accuracy\": Tensor(1),\n",
    "                          \"D_F_accuracy\": Tensor(1),\n",
    "                          \"D_R_accuracy\": Tensor(1),}\n",
    "\n",
    "for epoch in range(opt.starting_epoch, opt.total_epochs):\n",
    "    with tqdm.auto.tqdm(total=len(dataloader), unit=\"batch\", desc=f\"Epoch {epoch}\") as pbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            photo_base = batch[\"photo_base\"]\n",
    "            photo_struct = batch[\"photo_struct\"]\n",
    "            masks = batch[\"mask\"]\n",
    "\n",
    "            if should_run_discriminator(i, discriminator_freq):\n",
    "                disc_optimizer.zero_grad()\n",
    "\n",
    "                generated_f, losses, accuracies = network.real_branch_discriminator(\n",
    "                    photo_struct.to(device),\n",
    "                    photo_base.to(device),\n",
    "                    masks.to(device),\n",
    "                )\n",
    "\n",
    "                discriminator_loss = sum(losses.values())\n",
    "                discriminator_loss.backward()\n",
    "\n",
    "                disc_optimizer.step()\n",
    "\n",
    "                # dup fix me?\n",
    "                min_accuracy = accuracies[\"D_R_accuracy\"].item()\n",
    "                if min_accuracy < 0.8 and discriminator_freq < max_discriminator_freq:\n",
    "                    discriminator_freq *= 2\n",
    "                elif min_accuracy > 0.9 and discriminator_freq > min_discriminator_freq:\n",
    "                    discriminator_freq /= 2\n",
    "\n",
    "            else:\n",
    "                R_optimizer.zero_grad()\n",
    "                generated_r, losses, accuracies = network.real_branch_generator(\n",
    "                    photo_struct.to(device),\n",
    "                    photo_base.to(device),\n",
    "                    masks.to(device),\n",
    "                )\n",
    "                loss = sum(losses.values())\n",
    "                loss.backward()\n",
    "                R_optimizer.step()\n",
    "\n",
    "                min_accuracy = accuracies[\"D_R_accuracy\"].item()\n",
    "                if min_accuracy < 0.8 and discriminator_freq < max_discriminator_freq:\n",
    "                    discriminator_freq *= 2\n",
    "#                 if accuracies[\"D_EMB_accuracy\"] < 0.5 and discriminator_embedding_freq < max_discriminator_freq:\n",
    "#                     discriminator_embedding_freq *= 2\n",
    "\n",
    "                if i % 50 == 0:\n",
    "                    image_grid_to_writer(\n",
    "                        writer,\n",
    "                        torch.cat([generated_r.cpu(), photo_base]),\n",
    "                        \"images_r\",\n",
    "                        log_iter,\n",
    "                    )\n",
    "\n",
    "            for d in [losses, accuracies]:\n",
    "                for key, value in d.items():\n",
    "                    writer.add_scalar(key, value.item(), log_iter)\n",
    "#             writer.add_scalar(\"D_EMB_frequency\", float(discriminator_embedding_freq), log_iter)\n",
    "\n",
    "            pbar.set_postfix(discriminator_freq=str(discriminator_freq),\n",
    "                             min_accuracy=min_accuracy,\n",
    "#                              d_emb_freq=str(discriminator_embedding_freq),\n",
    "#                              d_emb_accuracy=accuracies[\"D_EMB_accuracy\"].item(),\n",
    "                            )\n",
    "            pbar.update(1)\n",
    "            log_iter += 1\n",
    "\n",
    "    if epoch % 2 == 1:\n",
    "        torch.save({\n",
    "            \"network_state_dict\": network.state_dict(),\n",
    "            \"log_iter\": log_iter,\n",
    "        }, f\"/home/andresokol/weights/v5_fix_HD_96_latent_only_R_{epoch}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
