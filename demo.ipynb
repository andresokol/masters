{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cuda_available = torch.cuda.is_available()\n",
    "# dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "# print(f\"Using {device} device\")\n",
    "# torch.cuda.empty_cache()\n",
    "Tensor = torch.Tensor\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "#     \"\"\"3x3 convolution with padding\"\"\"\n",
    "#     return nn.Conv2d(\n",
    "#         in_planes,\n",
    "#         out_planes,\n",
    "#         kernel_size=3,\n",
    "#         stride=stride,\n",
    "#         padding=dilation,\n",
    "#         groups=groups,\n",
    "#         bias=False,\n",
    "#         dilation=dilation,\n",
    "#     )\n",
    "#\n",
    "#\n",
    "# def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "#     \"\"\"1x1 convolution\"\"\"\n",
    "#     return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *(nn.Conv2d(3, 3, kernel_size=3, padding=1, stride=2) for _ in range(5))\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.layers.forward(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch version of ResNet\n",
    "# https://github.com/pytorch/vision/blob/cddad9ca3822011548e18342f52a3e9f4724c2dd/torchvision/models/resnet.py#L88\n",
    "\n",
    "\n",
    "class ResBlk(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(3)\n",
    "        self.conv_2 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(3)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.conv_1(x)\n",
    "        out = self.batch_norm_1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "\n",
    "        out += x\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "shared_layer_1, shared_layer_2 = ResBlk(), ResBlk()\n",
    "\n",
    "E_f = nn.Sequential(\n",
    "    DownsampleBlock(),\n",
    "    *(ResBlk() for _ in range(4)),\n",
    "    shared_layer_1,\n",
    "    shared_layer_2,\n",
    ").to(device)\n",
    "\n",
    "E_r = nn.Sequential(\n",
    "    DownsampleBlock(),\n",
    "    *(ResBlk() for _ in range(4)),\n",
    "    shared_layer_1,\n",
    "    shared_layer_2,\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class GeneratorFake(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resblk1 = ResBlk()\n",
    "        self.resblk2 = ResBlk()\n",
    "        self.upscale_layers = nn.Sequential(*(nn.ConvTranspose2d(3, 3, kernel_size=3,\n",
    "                                                                 stride=2,\n",
    "                                                                 padding=1,\n",
    "                                                                 output_padding=1,  # is this correct?\n",
    "                                                                 ) for _ in range(5)))\n",
    "\n",
    "        self.out_conv = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.resblk1(x)\n",
    "        out = self.resblk2(out)\n",
    "        out = self.upscale_layers(out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        # for i in range(5):\n",
    "        #     out = self.upscale_layers[i](out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        return self.out_conv(out)\n",
    "\n",
    "\n",
    "G_f = GeneratorFake().to(device)\n",
    "# G_f = nn.Sequential(\n",
    "#     *(ResBlk() for _ in range(2)),\n",
    "#     *(nn.ConvTranspose2d(3, 3, kernel_size=3, stride=2, padding=1) for _ in range(5)),\n",
    "#     # nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1),\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/stargan/models.py\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *discriminator_block(3, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1, bias=False),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img: Tensor) -> Tensor:\n",
    "        # combined = torch.cat((img_a, img_b), dim=1)\n",
    "        # out = img\n",
    "        # # print(\"Discriminator start\", out.shape)\n",
    "        #\n",
    "        # for layer in self.layers:\n",
    "        #     out = layer(out)\n",
    "        #     # print(layer, out.shape)\n",
    "\n",
    "        return self.layers(img)\n",
    "\n",
    "\n",
    "D_f = Discriminator().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# class EmbeddingDiscriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(3 * 16 * 16),\n",
    "#             nn.Linear()\n",
    "#         )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Options:\n",
    "    img_height = 512\n",
    "    img_width = 512\n",
    "    learning_rate = 0.0001  # from paper\n",
    "    starting_epoch = 0\n",
    "    total_epochs = 10\n",
    "    batch_size = 8\n",
    "    cpu_count = 4\n",
    "\n",
    "\n",
    "opt = Options()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "FILE_ROOT = \"/home/andresokol/code/mastersdata\"\n",
    "FFHQ_DIR = f\"{FILE_ROOT}/ffhq-dataset/images1024x1024\"\n",
    "PREPARED_ROOT = f\"{FILE_ROOT}/prepared\"\n",
    "RENDERED_ROOT = f\"{FILE_ROOT}/rendered\"\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((opt.img_height, opt.img_width), transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    def __init__(self, mode=\"train\"):\n",
    "        self.files = []\n",
    "\n",
    "        for i in range(70):\n",
    "            for j in range(1000):\n",
    "                part = f\"{i:02}000/{i:02}{j:03}\"\n",
    "                if os.path.exists(f\"{FFHQ_DIR}/{part}.png\"):\n",
    "                    if os.path.exists(f\"{RENDERED_ROOT}/{part}_base.png\"):\n",
    "                        if os.path.exists(f\"{RENDERED_ROOT}/{part}_structure.png\"):\n",
    "                            self.files.append(part)\n",
    "\n",
    "        print(f\"Read {len(self.files)} images\")\n",
    "        # self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
    "        # if mode == \"train\":\n",
    "        #     self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        part = self.files[index % len(self.files)]\n",
    "        render_base = Image.open(f\"{RENDERED_ROOT}/{part}_base.png\").convert(\"RGB\")\n",
    "        render_struct = Image.open(f\"{RENDERED_ROOT}/{part}_structure.png\").convert(\"RGB\")\n",
    "        # img = Image.open(self.files[index % len(self.files)])\n",
    "        # w, h = img.size\n",
    "        # img_A = img.crop((0, 0, w / 2, h))\n",
    "        # img_B = img.crop((w / 2, 0, w, h))\n",
    "\n",
    "        # if np.random.random() < 0.5:\n",
    "        #     img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "        #     img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "\n",
    "        # img_A = self.transform(img_A)\n",
    "        # img_B = self.transform(img_B)\n",
    "\n",
    "        return {\n",
    "            \"render_base\": self.transform(render_base),\n",
    "            \"render_struct\": self.transform(render_struct),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 7649 images\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(),\n",
    "    batch_size=opt.batch_size,\n",
    "    num_workers=opt.cpu_count,\n",
    "    shuffle=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "\n",
    "# eh?\n",
    "if opt.starting_epoch == 0:\n",
    "    E_f.apply(weights_init_normal)\n",
    "    G_f.apply(weights_init_normal)\n",
    "    D_f.apply(weights_init_normal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "optimizer_F = torch.optim.Adam([\n",
    "    {\"params\": E_f.parameters()},\n",
    "    {\"params\": G_f.parameters()},\n",
    "], lr=opt.learning_rate)\n",
    "optimizer_D_F = torch.optim.Adam(D_f.parameters(), lr=opt.learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch 0:   0%|          | 0/957 [00:00<?, ?batch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b4421fa31c2457fb3254a759636d3ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 16>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     38\u001B[0m optimizer_D_F\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     39\u001B[0m D_f_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mlogsigmoid(D_f(render_base))\u001B[38;5;241m.\u001B[39mmean() \u001B[38;5;241m-\u001B[39m functional\u001B[38;5;241m.\u001B[39mlogsigmoid(\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;241m-\u001B[39mD_f(generated_f))\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m---> 41\u001B[0m \u001B[43mD_f_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m optimizer_D_F\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     44\u001B[0m writer\u001B[38;5;241m.\u001B[39madd_scalar(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD_f loss\u001B[39m\u001B[38;5;124m'\u001B[39m, D_f_loss\u001B[38;5;241m.\u001B[39mitem(), i)\n",
      "File \u001B[0;32m~/code/masters/venv/lib/python3.8/site-packages/torch/_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/masters/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import tqdm\n",
    "from torch.nn import functional\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "# https://github.com/hse-ds/iad-applied-ds/blob/master/2022/seminars/sem03/sem03-gan-task.ipynb\n",
    "# https://colab.research.google.com/github/yandexdataschool/mlhep2019/blob/master/notebooks/day-6/06_GAN_faces_solution.ipynb#scrollTo=-Z7YAPGxp2Xl\n",
    "\n",
    "# branch_F_loss = Tensor([0])\n",
    "# D_f_loss = Tensor([0])\n",
    "\n",
    "branch_F_loss, D_f_loss = None, None\n",
    "\n",
    "for epoch in range(opt.starting_epoch, opt.total_epochs):\n",
    "    with tqdm.auto.tqdm(total=len(dataloader), unit=\"batch\", desc=f\"Epoch {epoch}\") as pbar:\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Model inputs\n",
    "            render_base = batch[\"render_base\"].to(device)\n",
    "            render_struct = batch[\"render_struct\"].to(device)\n",
    "\n",
    "            embedding = E_f(render_struct)\n",
    "            generated_f = G_f(embedding)\n",
    "\n",
    "            # pred_f_generated = D_f(generated_f)\n",
    "            # pred_f_truth = D_f(render_base)\n",
    "\n",
    "            if i % 2 == 0:\n",
    "                # Train Generator\n",
    "                optimizer_F.zero_grad()\n",
    "                branch_F_loss = -functional.logsigmoid(D_f(generated_f)).mean()\n",
    "                branch_F_loss.backward()\n",
    "                optimizer_F.step()\n",
    "\n",
    "                writer.add_scalar('Branch_F loss', branch_F_loss.item(), i)\n",
    "            else:\n",
    "                optimizer_D_F.zero_grad()\n",
    "                D_f_loss = -functional.logsigmoid(D_f(render_base)).mean() - functional.logsigmoid(\n",
    "                    -D_f(generated_f)).mean()\n",
    "                D_f_loss.backward()\n",
    "                optimizer_D_F.step()\n",
    "\n",
    "                writer.add_scalar('D_f loss', D_f_loss.item(), i)\n",
    "\n",
    "            grid = torchvision.utils.make_grid(render_struct)\n",
    "            writer.add_image('images', grid, i)\n",
    "            if branch_F_loss and D_f_loss:\n",
    "                pbar.set_postfix(loss_D_f=D_f_loss.item(), loss_F=branch_F_loss.item())\n",
    "            pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn.BatchNorm1d()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}